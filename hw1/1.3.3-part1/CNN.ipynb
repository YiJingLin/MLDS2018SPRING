{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## target\n",
    "task : img classification\n",
    "\n",
    "data : MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "EPOCH = 10\n",
    "BATCH_SIZE = 1000\n",
    "LR = 0.01\n",
    "DOWNLOAD_MNIST = True # 請求下載"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# download data\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root = './mnist',\n",
    "    train=True, # is training data\n",
    "    transform=torchvision.transforms.ToTensor(), # trans from numpy.ary to torch Tensor\n",
    "    download = DOWNLOAD_MNIST,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIvxAUoQfSIrwA0kRfiApwg8kRfiBpP4PdQK+Ne/X5oUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa51dda44e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot one example\n",
    "print(train_data.train_data.size())\n",
    "print(train_data.train_labels.size())\n",
    "\n",
    "plt.imshow(train_data.train_data[0].numpy(), cmap='gray')\n",
    "plt.title('%i' % train_data.train_labels[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = torchvision.datasets.MNIST(root='./mnist', train=False)\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = Variable(torch.unsqueeze(test_data.test_data, dim=1), volatile=True).type(torch.FloatTensor)\n",
    "test_x = test_x[:2000]/255.\n",
    "\n",
    "test_y = test_data.test_labels[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(# input size(1,28,28)\n",
    "            nn.Conv2d( \n",
    "                in_channels = 1, # gray(if RGB, then in_channel=3)\n",
    "                out_channels = 16, # num_filter\n",
    "                kernel_size=5, # filter's height and width = 5\n",
    "                stride=1, # shift 1 distance each time\n",
    "                padding=2, # if stride=1, padding=(kernel_size-1)/2, filled with zero\n",
    "            ),# -->(16, 28, 28)\n",
    "            nn.ReLU(),# -->(16, 28, 28)\n",
    "            nn.MaxPool2d(kernel_size=2,),# -->(16, 14, 14)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(# input size(16,14,14)\n",
    "            nn.Conv2d(16, 32, 5, 1, 2), #(in, out, kernel, strid, padding) # -->(32, 14, 14)\n",
    "            nn.ReLU(),# -->(32, 14, 14)\n",
    "            nn.MaxPool2d(2), # -->(32, 7, 7)\n",
    "        )\n",
    "        self.out = nn.Linear(32*7*7, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x) # (batch, 32, 7, 7)\n",
    "        x = x.view(x.size(0), -1) #(batch, 32*7*7)\n",
    "        output = self.out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN (\n",
      "  (conv1): Sequential (\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU ()\n",
      "    (2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  )\n",
      "  (conv2): Sequential (\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU ()\n",
      "    (2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  )\n",
      "  (out): Linear (1568 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn=CNN()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  | step  0  | train loss 2.3038  | test accuracy: 0.0895\n",
      "Epoch  0  | step  1  | train loss 2.4055  | test accuracy: 0.3020\n",
      "Epoch  0  | step  2  | train loss 2.1932  | test accuracy: 0.4850\n",
      "Epoch  0  | step  3  | train loss 2.0841  | test accuracy: 0.6850\n",
      "Epoch  0  | step  4  | train loss 1.7605  | test accuracy: 0.6725\n",
      "Epoch  0  | step  5  | train loss 1.3702  | test accuracy: 0.7465\n",
      "Epoch  0  | step  6  | train loss 0.9244  | test accuracy: 0.6680\n",
      "Epoch  0  | step  7  | train loss 0.8267  | test accuracy: 0.6345\n",
      "Epoch  0  | step  8  | train loss 0.9909  | test accuracy: 0.7060\n",
      "Epoch  0  | step  9  | train loss 0.7458  | test accuracy: 0.7510\n",
      "Epoch  0  | step  10  | train loss 1.1259  | test accuracy: 0.7395\n",
      "Epoch  0  | step  11  | train loss 1.0239  | test accuracy: 0.7560\n",
      "Epoch  0  | step  12  | train loss 0.7303  | test accuracy: 0.8190\n",
      "Epoch  0  | step  13  | train loss 0.3926  | test accuracy: 0.7860\n",
      "Epoch  0  | step  14  | train loss 0.5387  | test accuracy: 0.7805\n",
      "Epoch  0  | step  15  | train loss 0.6277  | test accuracy: 0.8430\n",
      "Epoch  0  | step  16  | train loss 0.4946  | test accuracy: 0.8690\n",
      "Epoch  0  | step  17  | train loss 0.4517  | test accuracy: 0.8665\n",
      "Epoch  0  | step  18  | train loss 0.4283  | test accuracy: 0.8605\n",
      "Epoch  0  | step  19  | train loss 0.4643  | test accuracy: 0.8660\n",
      "Epoch  0  | step  20  | train loss 0.4221  | test accuracy: 0.8795\n",
      "Epoch  0  | step  21  | train loss 0.3208  | test accuracy: 0.8875\n",
      "Epoch  0  | step  22  | train loss 0.2612  | test accuracy: 0.8955\n",
      "Epoch  0  | step  23  | train loss 0.2940  | test accuracy: 0.8955\n",
      "Epoch  0  | step  24  | train loss 0.3449  | test accuracy: 0.8975\n",
      "Epoch  0  | step  25  | train loss 0.3482  | test accuracy: 0.9070\n",
      "Epoch  0  | step  26  | train loss 0.2280  | test accuracy: 0.9165\n",
      "Epoch  0  | step  27  | train loss 0.2046  | test accuracy: 0.9230\n",
      "Epoch  0  | step  28  | train loss 0.2430  | test accuracy: 0.9260\n",
      "Epoch  0  | step  29  | train loss 0.2112  | test accuracy: 0.9230\n",
      "Epoch  0  | step  30  | train loss 0.2359  | test accuracy: 0.9220\n",
      "Epoch  0  | step  31  | train loss 0.2423  | test accuracy: 0.9200\n",
      "Epoch  0  | step  32  | train loss 0.2702  | test accuracy: 0.9205\n",
      "Epoch  0  | step  33  | train loss 0.1755  | test accuracy: 0.9240\n",
      "Epoch  0  | step  34  | train loss 0.1673  | test accuracy: 0.9260\n",
      "Epoch  0  | step  35  | train loss 0.1999  | test accuracy: 0.9285\n",
      "Epoch  0  | step  36  | train loss 0.1964  | test accuracy: 0.9310\n",
      "Epoch  0  | step  37  | train loss 0.1645  | test accuracy: 0.9335\n",
      "Epoch  0  | step  38  | train loss 0.2411  | test accuracy: 0.9370\n",
      "Epoch  0  | step  39  | train loss 0.2498  | test accuracy: 0.9390\n",
      "Epoch  0  | step  40  | train loss 0.1903  | test accuracy: 0.9440\n",
      "Epoch  0  | step  41  | train loss 0.1885  | test accuracy: 0.9465\n",
      "Epoch  0  | step  42  | train loss 0.1210  | test accuracy: 0.9435\n",
      "Epoch  0  | step  43  | train loss 0.1786  | test accuracy: 0.9395\n",
      "Epoch  0  | step  44  | train loss 0.1279  | test accuracy: 0.9360\n",
      "Epoch  0  | step  45  | train loss 0.1764  | test accuracy: 0.9425\n",
      "Epoch  0  | step  46  | train loss 0.1508  | test accuracy: 0.9455\n",
      "Epoch  0  | step  47  | train loss 0.1401  | test accuracy: 0.9445\n",
      "Epoch  0  | step  48  | train loss 0.1242  | test accuracy: 0.9450\n",
      "Epoch  0  | step  49  | train loss 0.1391  | test accuracy: 0.9450\n",
      "Epoch  0  | step  50  | train loss 0.1566  | test accuracy: 0.9480\n",
      "Epoch  0  | step  51  | train loss 0.2253  | test accuracy: 0.9490\n",
      "Epoch  0  | step  52  | train loss 0.1223  | test accuracy: 0.9500\n",
      "Epoch  0  | step  53  | train loss 0.1472  | test accuracy: 0.9455\n",
      "Epoch  0  | step  54  | train loss 0.1438  | test accuracy: 0.9485\n",
      "Epoch  0  | step  55  | train loss 0.1414  | test accuracy: 0.9520\n",
      "Epoch  0  | step  56  | train loss 0.1222  | test accuracy: 0.9570\n",
      "Epoch  0  | step  57  | train loss 0.1293  | test accuracy: 0.9580\n",
      "Epoch  0  | step  58  | train loss 0.1165  | test accuracy: 0.9545\n",
      "Epoch  0  | step  59  | train loss 0.1052  | test accuracy: 0.9485\n",
      "Epoch  0  | step  60  | train loss 0.1611  | test accuracy: 0.9570\n",
      "Epoch  0  | step  61  | train loss 0.1428  | test accuracy: 0.9625\n",
      "Epoch  0  | step  62  | train loss 0.1329  | test accuracy: 0.9630\n",
      "Epoch  0  | step  63  | train loss 0.0807  | test accuracy: 0.9605\n",
      "Epoch  0  | step  64  | train loss 0.0987  | test accuracy: 0.9610\n",
      "Epoch  0  | step  65  | train loss 0.1029  | test accuracy: 0.9605\n",
      "Epoch  0  | step  66  | train loss 0.1582  | test accuracy: 0.9595\n",
      "Epoch  0  | step  67  | train loss 0.1291  | test accuracy: 0.9605\n",
      "Epoch  0  | step  68  | train loss 0.1585  | test accuracy: 0.9615\n",
      "Epoch  0  | step  69  | train loss 0.1280  | test accuracy: 0.9590\n",
      "Epoch  0  | step  70  | train loss 0.0794  | test accuracy: 0.9600\n",
      "Epoch  0  | step  71  | train loss 0.1642  | test accuracy: 0.9605\n",
      "Epoch  0  | step  72  | train loss 0.0894  | test accuracy: 0.9610\n",
      "Epoch  0  | step  73  | train loss 0.1386  | test accuracy: 0.9620\n",
      "Epoch  0  | step  74  | train loss 0.0948  | test accuracy: 0.9615\n",
      "Epoch  0  | step  75  | train loss 0.1377  | test accuracy: 0.9615\n",
      "Epoch  0  | step  76  | train loss 0.1261  | test accuracy: 0.9625\n",
      "Epoch  0  | step  77  | train loss 0.0795  | test accuracy: 0.9640\n",
      "Epoch  0  | step  78  | train loss 0.1043  | test accuracy: 0.9635\n",
      "Epoch  0  | step  79  | train loss 0.1125  | test accuracy: 0.9645\n",
      "Epoch  0  | step  80  | train loss 0.1180  | test accuracy: 0.9650\n",
      "Epoch  0  | step  81  | train loss 0.0736  | test accuracy: 0.9630\n",
      "Epoch  0  | step  82  | train loss 0.1104  | test accuracy: 0.9630\n",
      "Epoch  0  | step  83  | train loss 0.1160  | test accuracy: 0.9635\n",
      "Epoch  0  | step  84  | train loss 0.0732  | test accuracy: 0.9635\n",
      "Epoch  0  | step  85  | train loss 0.1071  | test accuracy: 0.9655\n",
      "Epoch  0  | step  86  | train loss 0.0766  | test accuracy: 0.9645\n",
      "Epoch  0  | step  87  | train loss 0.0755  | test accuracy: 0.9675\n",
      "Epoch  0  | step  88  | train loss 0.0723  | test accuracy: 0.9675\n",
      "Epoch  0  | step  89  | train loss 0.0824  | test accuracy: 0.9680\n",
      "Epoch  0  | step  90  | train loss 0.0648  | test accuracy: 0.9670\n",
      "Epoch  0  | step  91  | train loss 0.0884  | test accuracy: 0.9660\n",
      "Epoch  0  | step  92  | train loss 0.1083  | test accuracy: 0.9685\n",
      "Epoch  0  | step  93  | train loss 0.0688  | test accuracy: 0.9700\n",
      "Epoch  0  | step  94  | train loss 0.1057  | test accuracy: 0.9695\n",
      "Epoch  0  | step  95  | train loss 0.0820  | test accuracy: 0.9700\n",
      "Epoch  0  | step  96  | train loss 0.1331  | test accuracy: 0.9710\n",
      "Epoch  0  | step  97  | train loss 0.0518  | test accuracy: 0.9705\n",
      "Epoch  0  | step  98  | train loss 0.1408  | test accuracy: 0.9685\n",
      "Epoch  0  | step  99  | train loss 0.0703  | test accuracy: 0.9690\n",
      "Epoch  0  | step  100  | train loss 0.0480  | test accuracy: 0.9690\n",
      "Epoch  0  | step  101  | train loss 0.0864  | test accuracy: 0.9665\n",
      "Epoch  0  | step  102  | train loss 0.0705  | test accuracy: 0.9700\n",
      "Epoch  0  | step  103  | train loss 0.0585  | test accuracy: 0.9695\n",
      "Epoch  0  | step  104  | train loss 0.1234  | test accuracy: 0.9725\n",
      "Epoch  0  | step  105  | train loss 0.0754  | test accuracy: 0.9735\n",
      "Epoch  0  | step  106  | train loss 0.1025  | test accuracy: 0.9740\n",
      "Epoch  0  | step  107  | train loss 0.0815  | test accuracy: 0.9740\n",
      "Epoch  0  | step  108  | train loss 0.0700  | test accuracy: 0.9755\n",
      "Epoch  0  | step  109  | train loss 0.0948  | test accuracy: 0.9745\n",
      "Epoch  0  | step  110  | train loss 0.0909  | test accuracy: 0.9750\n",
      "Epoch  0  | step  111  | train loss 0.1080  | test accuracy: 0.9750\n",
      "Epoch  0  | step  112  | train loss 0.0835  | test accuracy: 0.9735\n",
      "Epoch  0  | step  113  | train loss 0.0915  | test accuracy: 0.9705\n",
      "Epoch  0  | step  114  | train loss 0.0696  | test accuracy: 0.9705\n",
      "Epoch  0  | step  115  | train loss 0.1311  | test accuracy: 0.9710\n",
      "Epoch  0  | step  116  | train loss 0.0846  | test accuracy: 0.9710\n",
      "Epoch  0  | step  117  | train loss 0.0778  | test accuracy: 0.9750\n",
      "Epoch  0  | step  118  | train loss 0.0897  | test accuracy: 0.9720\n",
      "Epoch  0  | step  119  | train loss 0.0859  | test accuracy: 0.9705\n",
      "Epoch  1  | step  0  | train loss 0.0474  | test accuracy: 0.9720\n",
      "Epoch  1  | step  1  | train loss 0.0835  | test accuracy: 0.9750\n",
      "Epoch  1  | step  2  | train loss 0.1257  | test accuracy: 0.9750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1  | step  3  | train loss 0.0987  | test accuracy: 0.9800\n",
      "Epoch  1  | step  4  | train loss 0.0971  | test accuracy: 0.9785\n",
      "Epoch  1  | step  5  | train loss 0.0970  | test accuracy: 0.9775\n",
      "Epoch  1  | step  6  | train loss 0.0904  | test accuracy: 0.9740\n",
      "Epoch  1  | step  7  | train loss 0.0538  | test accuracy: 0.9740\n",
      "Epoch  1  | step  8  | train loss 0.0617  | test accuracy: 0.9715\n",
      "Epoch  1  | step  9  | train loss 0.0589  | test accuracy: 0.9710\n",
      "Epoch  1  | step  10  | train loss 0.0521  | test accuracy: 0.9690\n",
      "Epoch  1  | step  11  | train loss 0.0874  | test accuracy: 0.9670\n",
      "Epoch  1  | step  12  | train loss 0.0800  | test accuracy: 0.9690\n",
      "Epoch  1  | step  13  | train loss 0.1039  | test accuracy: 0.9700\n",
      "Epoch  1  | step  14  | train loss 0.0694  | test accuracy: 0.9745\n",
      "Epoch  1  | step  15  | train loss 0.0517  | test accuracy: 0.9745\n",
      "Epoch  1  | step  16  | train loss 0.0646  | test accuracy: 0.9740\n",
      "Epoch  1  | step  17  | train loss 0.0503  | test accuracy: 0.9735\n",
      "Epoch  1  | step  18  | train loss 0.0835  | test accuracy: 0.9755\n",
      "Epoch  1  | step  19  | train loss 0.0732  | test accuracy: 0.9735\n",
      "Epoch  1  | step  20  | train loss 0.0626  | test accuracy: 0.9765\n",
      "Epoch  1  | step  21  | train loss 0.0634  | test accuracy: 0.9745\n",
      "Epoch  1  | step  22  | train loss 0.0580  | test accuracy: 0.9725\n",
      "Epoch  1  | step  23  | train loss 0.0242  | test accuracy: 0.9715\n",
      "Epoch  1  | step  24  | train loss 0.0603  | test accuracy: 0.9730\n",
      "Epoch  1  | step  25  | train loss 0.1027  | test accuracy: 0.9725\n",
      "Epoch  1  | step  26  | train loss 0.0766  | test accuracy: 0.9745\n",
      "Epoch  1  | step  27  | train loss 0.0459  | test accuracy: 0.9750\n",
      "Epoch  1  | step  28  | train loss 0.1035  | test accuracy: 0.9750\n",
      "Epoch  1  | step  29  | train loss 0.0788  | test accuracy: 0.9740\n",
      "Epoch  1  | step  30  | train loss 0.0437  | test accuracy: 0.9750\n",
      "Epoch  1  | step  31  | train loss 0.0539  | test accuracy: 0.9760\n",
      "Epoch  1  | step  32  | train loss 0.0717  | test accuracy: 0.9760\n",
      "Epoch  1  | step  33  | train loss 0.0890  | test accuracy: 0.9765\n",
      "Epoch  1  | step  34  | train loss 0.0660  | test accuracy: 0.9790\n",
      "Epoch  1  | step  35  | train loss 0.0733  | test accuracy: 0.9780\n",
      "Epoch  1  | step  36  | train loss 0.0902  | test accuracy: 0.9740\n",
      "Epoch  1  | step  37  | train loss 0.0892  | test accuracy: 0.9755\n",
      "Epoch  1  | step  38  | train loss 0.0409  | test accuracy: 0.9735\n",
      "Epoch  1  | step  39  | train loss 0.0705  | test accuracy: 0.9730\n",
      "Epoch  1  | step  40  | train loss 0.0685  | test accuracy: 0.9740\n",
      "Epoch  1  | step  41  | train loss 0.0765  | test accuracy: 0.9775\n",
      "Epoch  1  | step  42  | train loss 0.0621  | test accuracy: 0.9790\n",
      "Epoch  1  | step  43  | train loss 0.0569  | test accuracy: 0.9790\n",
      "Epoch  1  | step  44  | train loss 0.0414  | test accuracy: 0.9790\n",
      "Epoch  1  | step  45  | train loss 0.0970  | test accuracy: 0.9775\n",
      "Epoch  1  | step  46  | train loss 0.0846  | test accuracy: 0.9800\n",
      "Epoch  1  | step  47  | train loss 0.0642  | test accuracy: 0.9805\n"
     ]
    }
   ],
   "source": [
    "target = False # upto 98% or not\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    if target: break\n",
    "    for step, (x, y) in enumerate(train_loader):\n",
    "        b_x = Variable(x, requires_grad=True) # batch x\n",
    "        b_y = Variable(y) # batch y\n",
    "        \n",
    "        output = cnn(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        test_output = cnn(test_x)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "        accuracy = sum(pred_y ==test_y)/ test_y.size(0)\n",
    "                \n",
    "\n",
    "        print('Epoch ', epoch, ' | step ', step, \n",
    "              ' | train loss %.4f'%loss.data[0], \n",
    "              ' | test accuracy: %.4f'%accuracy,\n",
    "#               ' | gradient of loss to input ', frobenius_norm,\n",
    "                 )\n",
    "        if accuracy > 0.98:\n",
    "            target = True\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### train\n",
    "# train_loss\n",
    "loss.data[0]\n",
    "# train_acc\n",
    "train_x = Variable(torch.unsqueeze(train_data.train_data, dim=1), volatile=True).type(torch.FloatTensor)/255.\n",
    "train_prediction = cnn.forward(train_x)\n",
    "train_pred = torch.max(train_prediction, 1)[1].data.squeeze()\n",
    "train_y = train_data.train_labels\n",
    "train_acc = sum(train_pred ==train_y)/ train_y.size(0)\n",
    "\n",
    "### test\n",
    "test_pred = cnn.forward(test_x)\n",
    "\n",
    "# test_loss\n",
    "test_loss = loss_func(test_pred, Variable(test_y))\n",
    "# test_acc\n",
    "test_pred = torch.max(test_pred, 1)[1].data.squeeze()\n",
    "test_acc = sum(test_pred ==test_y)/ test_y.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Batch size : %s \\n' % str(BATCH_SIZE))\n",
    "\n",
    "\n",
    "# training data\n",
    "print('train_loss %.4f' % loss.data[0]) # training loss\n",
    "print('train_acc %.4f' % train_acc)\n",
    "print('test_loss %.4f' % test_loss.data[0])\n",
    "print('test_acc %.4f' % test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.0.weight', \n",
       "              (0 ,0 ,.,.) = \n",
       "                0.0158  0.0403  0.0528  0.0827  0.1460\n",
       "               -0.1770  0.2452  0.1085  0.0891 -0.1623\n",
       "                0.1601  0.2556  0.1772 -0.2450 -0.3134\n",
       "                0.1226  0.2723 -0.1509 -0.1824  0.0640\n",
       "               -0.1373  0.1742  0.0582 -0.0382  0.0155\n",
       "              \n",
       "              (1 ,0 ,.,.) = \n",
       "                0.1263  0.1729 -0.0200  0.2149  0.1740\n",
       "                0.2587  0.2947  0.3108 -0.0563  0.2721\n",
       "               -0.1399  0.1162 -0.0232  0.1300 -0.0653\n",
       "               -0.2741 -0.2213 -0.0656  0.0635  0.0107\n",
       "               -0.3065 -0.2012 -0.3409 -0.1871 -0.0606\n",
       "              \n",
       "              (2 ,0 ,.,.) = \n",
       "               -0.2520 -0.0662  0.1957 -0.0973 -0.0200\n",
       "               -0.2052  0.1190  0.0547  0.2190 -0.2584\n",
       "               -0.4580  0.1426  0.2589 -0.0570 -0.1024\n",
       "               -0.2395 -0.1034  0.2593  0.1226 -0.0681\n",
       "               -0.2993 -0.0842  0.2346  0.2295 -0.1213\n",
       "              \n",
       "              (3 ,0 ,.,.) = \n",
       "                0.0133  0.1581  0.1153  0.2004 -0.2097\n",
       "               -0.1580 -0.1764  0.0429  0.2090  0.2107\n",
       "               -0.3525  0.0975 -0.1187  0.0216  0.1756\n",
       "               -0.0599 -0.2129 -0.1190 -0.0623  0.1636\n",
       "                0.0883 -0.2002 -0.1009 -0.1912 -0.2817\n",
       "              \n",
       "              (4 ,0 ,.,.) = \n",
       "                0.0813 -0.2043 -0.2354 -0.2579 -0.1433\n",
       "               -0.3978 -0.3780 -0.2988 -0.0735  0.1452\n",
       "               -0.2879 -0.1551 -0.1227 -0.0027  0.0191\n",
       "                0.2356 -0.0524  0.2612  0.3025  0.0078\n",
       "                0.1283  0.2692 -0.0348  0.2476  0.0475\n",
       "              \n",
       "              (5 ,0 ,.,.) = \n",
       "               -0.0833  0.2272  0.0453 -0.2582  0.0163\n",
       "                0.1648  0.1022  0.2197  0.1733  0.1135\n",
       "               -0.0600 -0.1193  0.0804 -0.1733  0.0858\n",
       "               -0.1201  0.1358  0.2073 -0.1089 -0.1095\n",
       "                0.0611  0.1104  0.1092 -0.2281  0.0337\n",
       "              \n",
       "              (6 ,0 ,.,.) = \n",
       "                0.2516 -0.3312 -0.5459 -0.2563  0.0371\n",
       "                0.1094 -0.2474 -0.2290  0.0625  0.1588\n",
       "               -0.0032 -0.1662 -0.1928  0.2107  0.1504\n",
       "               -0.1863 -0.0597 -0.0711  0.3305 -0.1650\n",
       "               -0.1560 -0.0388  0.2603  0.1034 -0.3562\n",
       "              \n",
       "              (7 ,0 ,.,.) = \n",
       "               -0.0745  0.2267  0.0885 -0.0658 -0.0461\n",
       "                0.0653  0.0735  0.1987  0.1484 -0.2165\n",
       "               -0.1117  0.2230  0.0455  0.1160 -0.0784\n",
       "                0.0911  0.2606 -0.0798  0.0361 -0.1618\n",
       "                0.0715  0.0640  0.2272  0.2374 -0.1574\n",
       "              \n",
       "              (8 ,0 ,.,.) = \n",
       "               -0.2271 -0.4638 -0.3267 -0.2659 -0.0635\n",
       "                0.2473 -0.0008 -0.1677 -0.1648 -0.2608\n",
       "                0.2989  0.0052  0.1800 -0.1416  0.1802\n",
       "               -0.0995  0.1455  0.0714  0.2994  0.0064\n",
       "                0.1839  0.1795 -0.0856  0.1450  0.1381\n",
       "              \n",
       "              (9 ,0 ,.,.) = \n",
       "               -0.0286  0.1247  0.2653  0.0771  0.2320\n",
       "               -0.0303  0.1008  0.1795  0.3394  0.0737\n",
       "                0.3158  0.0279  0.0092 -0.1086  0.2383\n",
       "               -0.0178 -0.1490 -0.1797 -0.2315 -0.1035\n",
       "               -0.3131 -0.3485 -0.2752 -0.0686 -0.0670\n",
       "              \n",
       "              (10,0 ,.,.) = \n",
       "                0.2137 -0.1277  0.0048 -0.1026 -0.1838\n",
       "                0.2491  0.0561  0.1076  0.1024  0.1841\n",
       "               -0.0111  0.2560  0.1963  0.0694  0.2502\n",
       "               -0.2935  0.1399  0.0258 -0.0563  0.1450\n",
       "               -0.0730 -0.3465 -0.2338 -0.1383 -0.1586\n",
       "              \n",
       "              (11,0 ,.,.) = \n",
       "               -0.2365  0.0821  0.0304 -0.1497 -0.2414\n",
       "               -0.2126 -0.1216 -0.1149 -0.2805 -0.0322\n",
       "                0.1468  0.0835 -0.0855 -0.2859 -0.0617\n",
       "                0.2423 -0.1487  0.0810  0.0524 -0.0848\n",
       "                0.1238  0.2307  0.0576 -0.2222 -0.2078\n",
       "              \n",
       "              (12,0 ,.,.) = \n",
       "                0.2642  0.2788  0.2582  0.0961 -0.2461\n",
       "                0.1771  0.0208 -0.0221 -0.0090 -0.3711\n",
       "                0.2348 -0.0944 -0.0257 -0.3222 -0.2519\n",
       "               -0.0218  0.0429 -0.0162 -0.3681 -0.2581\n",
       "                0.1840 -0.0549 -0.4369 -0.1687 -0.1581\n",
       "              \n",
       "              (13,0 ,.,.) = \n",
       "                0.1291  0.1898  0.0119 -0.0501  0.0426\n",
       "                0.1291  0.0287  0.0378  0.0765 -0.2423\n",
       "                0.2575  0.2471  0.1484 -0.2103 -0.1573\n",
       "                0.2437  0.1260 -0.1792 -0.1760 -0.0656\n",
       "                0.1474  0.2161 -0.0868 -0.1337  0.0546\n",
       "              \n",
       "              (14,0 ,.,.) = \n",
       "                0.2413  0.1363  0.1660  0.1689  0.2777\n",
       "                0.2058 -0.0502 -0.1112  0.1196  0.0009\n",
       "               -0.0302  0.1033 -0.1498 -0.2514 -0.1162\n",
       "                0.0917 -0.2431 -0.2620 -0.1918 -0.0898\n",
       "                0.1111  0.0646 -0.0004  0.0911 -0.0547\n",
       "              \n",
       "              (15,0 ,.,.) = \n",
       "               -0.0409  0.1510 -0.1686  0.0717 -0.1193\n",
       "                0.1059 -0.1443 -0.0285  0.0982  0.2798\n",
       "               -0.1809  0.0744  0.0900  0.1887  0.1311\n",
       "               -0.3176 -0.2113 -0.0790 -0.1283 -0.1194\n",
       "               -0.1087 -0.0253 -0.3459 -0.2109 -0.2705\n",
       "              [torch.FloatTensor of size 16x1x5x5]), ('conv1.0.bias', \n",
       "              -0.1852\n",
       "               0.0114\n",
       "              -0.0081\n",
       "              -0.0241\n",
       "              -0.0026\n",
       "              -0.1270\n",
       "              -0.0078\n",
       "              -0.1699\n",
       "              -0.0003\n",
       "               0.0017\n",
       "              -0.1455\n",
       "               0.0378\n",
       "               0.0025\n",
       "              -0.1974\n",
       "              -0.0041\n",
       "               0.0271\n",
       "              [torch.FloatTensor of size 16]), ('conv2.0.weight', \n",
       "              (0 ,0 ,.,.) = \n",
       "               -1.0375e-01 -1.0659e-01 -2.5112e-02 -4.1583e-02 -1.2184e-01\n",
       "               -8.2640e-02 -3.2230e-02 -2.6052e-02 -3.7141e-02  6.2188e-03\n",
       "               -6.7313e-02 -3.7557e-02 -1.2547e-01  3.4746e-02  3.9353e-02\n",
       "               -6.8246e-02 -6.3387e-02 -1.9825e-02  2.4182e-02 -4.5177e-02\n",
       "               -7.6352e-02 -6.1495e-02  7.2375e-02 -2.8062e-04  6.5528e-02\n",
       "              \n",
       "              (0 ,1 ,.,.) = \n",
       "                4.5910e-03 -1.1349e-02  6.3614e-03 -3.9876e-02 -8.2743e-02\n",
       "               -1.1393e-01 -2.8452e-02 -1.1927e-01 -8.6272e-02 -3.6427e-02\n",
       "               -3.9119e-02 -1.3034e-01 -1.2707e-01 -3.9447e-02 -2.0696e-02\n",
       "               -5.8236e-02 -7.8533e-02 -1.2866e-01 -1.3444e-01 -4.8159e-02\n",
       "               -5.2393e-02 -3.3142e-02 -2.1589e-02  2.0807e-03 -2.4603e-02\n",
       "              \n",
       "              (0 ,2 ,.,.) = \n",
       "               -1.0395e-01 -1.1472e-01 -1.1582e-01 -7.6896e-02 -1.1762e-01\n",
       "               -4.5930e-02 -9.3839e-02 -9.6332e-02 -8.2782e-02 -1.1079e-01\n",
       "               -5.0236e-02 -1.9725e-02  1.8741e-02 -4.0661e-02 -9.4154e-02\n",
       "               -1.2885e-01 -3.0696e-02 -3.7242e-03 -3.6100e-02 -1.2144e-01\n",
       "               -5.2884e-02  2.2033e-02 -1.1472e-01 -5.8022e-02 -8.5883e-02\n",
       "                 ...\n",
       "              \n",
       "              (0 ,13,.,.) = \n",
       "               -5.2926e-02 -4.2743e-02  1.6611e-02  3.5576e-02 -3.6019e-02\n",
       "               -4.2224e-02 -7.4028e-02 -9.8542e-02  6.5328e-02  1.3641e-02\n",
       "               -5.7280e-02 -2.2420e-02 -8.6786e-02 -2.5147e-02 -1.5811e-02\n",
       "               -6.0895e-02 -1.1791e-01  6.7390e-02  2.6796e-02  1.0655e-02\n",
       "               -9.4164e-02 -4.0756e-02  1.3178e-02  7.9074e-02  5.4845e-02\n",
       "              \n",
       "              (0 ,14,.,.) = \n",
       "               -5.6309e-02 -6.0634e-02 -5.3009e-02 -5.5764e-02 -1.2336e-01\n",
       "               -1.0079e-01 -1.1299e-01 -1.0552e-01  2.5416e-02 -8.3165e-02\n",
       "               -1.1489e-01 -4.8690e-02 -1.0234e-01 -8.8985e-02 -1.1672e-01\n",
       "               -8.1376e-02 -8.0114e-02 -7.0299e-02 -5.4664e-02 -3.8426e-02\n",
       "               -4.8894e-02 -6.3759e-02  1.9329e-03  1.9012e-02 -6.0565e-02\n",
       "              \n",
       "              (0 ,15,.,.) = \n",
       "               -1.3070e-01 -8.0101e-02 -1.0615e-01 -7.4888e-02 -6.4876e-02\n",
       "               -1.0021e-01 -1.2398e-01 -6.0853e-02 -1.2721e-01 -1.1438e-01\n",
       "               -4.7090e-02 -3.1696e-02 -3.5477e-02 -3.2755e-02 -8.2272e-02\n",
       "               -4.4132e-02 -8.3161e-02 -1.1279e-01 -1.2649e-01 -8.0292e-02\n",
       "               -1.3451e-01 -2.8852e-02 -1.1000e-01 -4.8391e-02 -9.6020e-02\n",
       "                   ⋮ \n",
       "              \n",
       "              (1 ,0 ,.,.) = \n",
       "                8.4412e-02  6.7070e-02 -2.2954e-02 -7.6879e-02 -3.6292e-02\n",
       "                1.2340e-02 -8.7783e-03 -1.1898e-02 -1.1886e-01  5.7706e-03\n",
       "                6.9778e-02 -1.6044e-02 -7.4851e-02 -1.0020e-01  1.4479e-02\n",
       "               -1.1242e-01 -7.8596e-02 -3.9573e-02 -5.4082e-02 -7.3683e-02\n",
       "               -1.2532e-02 -7.1792e-02 -5.9344e-02 -5.5676e-02  8.7435e-03\n",
       "              \n",
       "              (1 ,1 ,.,.) = \n",
       "                8.1083e-02 -1.4472e-01 -2.6005e-02 -1.9224e-02  1.9729e-03\n",
       "                3.5694e-02 -3.8715e-02 -7.6374e-02 -3.8122e-02 -2.2593e-03\n",
       "               -3.0089e-02  9.5150e-03 -2.7682e-02 -4.5853e-02 -4.9561e-02\n",
       "               -3.7784e-02 -9.2676e-02 -3.1002e-02 -8.4453e-02 -5.8813e-02\n",
       "               -7.2508e-02 -4.2304e-02 -8.6194e-02 -4.9847e-02 -1.1118e-01\n",
       "              \n",
       "              (1 ,2 ,.,.) = \n",
       "               -2.3122e-02 -5.7814e-02 -4.0290e-02 -8.2056e-02 -5.9373e-02\n",
       "               -8.3293e-02  5.1454e-03 -6.3282e-02 -2.0064e-02 -2.0003e-04\n",
       "               -4.9862e-02 -4.4518e-02 -6.3234e-02 -1.6273e-02 -4.3291e-02\n",
       "               -3.4856e-02 -7.0964e-02 -6.5287e-02 -5.9342e-02 -3.2999e-02\n",
       "               -4.3967e-02 -6.7047e-02 -1.2955e-01 -2.0493e-02 -3.4569e-03\n",
       "                 ...\n",
       "              \n",
       "              (1 ,13,.,.) = \n",
       "                6.4916e-02  3.0822e-03  3.9842e-02 -7.3983e-02  9.2683e-03\n",
       "                2.8375e-02  1.8435e-02 -1.0152e-03 -8.8600e-02  3.0892e-02\n",
       "               -3.3276e-02 -7.6962e-02 -9.1909e-02 -2.0872e-02 -7.1110e-02\n",
       "               -7.0317e-02 -5.2215e-02 -3.9700e-02 -1.0607e-01 -8.9449e-02\n",
       "               -9.3877e-02 -8.9302e-02 -1.0262e-01 -6.0087e-02 -2.7922e-03\n",
       "              \n",
       "              (1 ,14,.,.) = \n",
       "               -4.2627e-02 -2.2395e-02  1.5999e-02 -5.5810e-02 -9.3618e-02\n",
       "               -4.0031e-02 -3.5913e-02 -4.8239e-02 -5.6104e-02 -6.1504e-02\n",
       "               -2.3161e-02 -5.6090e-02 -4.1284e-02 -3.7059e-02 -5.8180e-02\n",
       "               -4.6219e-02 -7.9584e-02 -3.7840e-02 -5.1256e-02 -1.2914e-01\n",
       "               -4.4569e-02 -7.5583e-02 -5.6843e-02 -9.3649e-02 -6.8260e-02\n",
       "              \n",
       "              (1 ,15,.,.) = \n",
       "               -5.8473e-02 -8.4303e-02 -4.8002e-02 -5.3364e-02 -3.7546e-02\n",
       "               -8.8140e-02 -1.0554e-01 -1.1397e-01 -3.3699e-02 -2.8728e-02\n",
       "               -5.3256e-02 -5.9776e-02 -7.1854e-02 -4.5370e-02 -3.8297e-02\n",
       "               -4.7801e-02 -1.2108e-01 -1.0178e-01 -1.0990e-01 -1.1806e-01\n",
       "               -3.3925e-02 -3.9467e-02 -9.0360e-02 -1.3101e-01 -4.6627e-02\n",
       "                   ⋮ \n",
       "              \n",
       "              (2 ,0 ,.,.) = \n",
       "                1.1946e-02 -9.6751e-02 -1.1979e-01 -9.8477e-02  1.0106e-02\n",
       "               -4.0891e-02 -2.9628e-02  5.0300e-02  2.6476e-02  3.8051e-02\n",
       "               -3.4758e-02  3.9203e-02  1.1653e-01  5.7446e-02  5.9563e-02\n",
       "               -2.0159e-02  7.0993e-02 -1.4324e-02 -1.2122e-02  8.8066e-02\n",
       "               -7.2404e-02 -4.6916e-02  2.2912e-02  4.2042e-02  7.4923e-02\n",
       "              \n",
       "              (2 ,1 ,.,.) = \n",
       "               -8.1191e-02 -1.3067e-01 -1.9383e-01 -1.2542e-01 -1.1407e-02\n",
       "               -1.4066e-01 -1.8014e-01 -1.4920e-01 -6.6039e-02  3.4467e-02\n",
       "                6.6459e-02 -6.6467e-02 -5.3093e-02 -2.7319e-02 -1.1529e-01\n",
       "                4.9340e-02  1.4345e-01  8.0270e-02  7.2734e-02  5.8330e-02\n",
       "                6.3667e-02  1.1355e-01  1.1164e-01  1.2869e-01  1.7783e-01\n",
       "              \n",
       "              (2 ,2 ,.,.) = \n",
       "               -4.5660e-02 -1.0044e-01  6.5677e-03 -7.3200e-02 -3.0066e-03\n",
       "                2.1708e-02  3.3916e-02  1.7158e-02  1.3667e-02 -1.8471e-02\n",
       "                3.6254e-02 -1.0539e-02  2.6107e-02  4.6697e-02 -1.2595e-02\n",
       "               -1.5921e-02 -1.0912e-02 -1.2759e-02  1.4851e-02  1.3899e-02\n",
       "               -1.1084e-01 -8.8770e-03 -1.6748e-01 -1.3553e-01 -1.0377e-04\n",
       "                 ...\n",
       "              \n",
       "              (2 ,13,.,.) = \n",
       "               -1.6832e-02 -6.1009e-02 -1.3099e-01 -6.3447e-02 -1.1187e-01\n",
       "                1.6915e-02 -2.2952e-02  1.3287e-02  2.6120e-02 -5.0078e-02\n",
       "               -1.2757e-01  1.2266e-02 -1.4487e-02 -1.0218e-02 -4.6130e-02\n",
       "               -5.9567e-02 -2.4863e-02 -5.8082e-02 -3.5619e-03 -4.0790e-02\n",
       "               -8.6877e-02 -1.1600e-01 -4.4358e-02 -2.7955e-02 -3.1138e-02\n",
       "              \n",
       "              (2 ,14,.,.) = \n",
       "               -3.7448e-02 -5.9643e-02 -1.5086e-01 -1.3426e-01 -1.4194e-01\n",
       "               -9.0107e-02 -8.2355e-02 -9.1005e-02  3.8853e-02  6.6521e-03\n",
       "               -1.0033e-01 -1.0185e-01  1.5550e-02 -4.3240e-02  3.0095e-02\n",
       "               -1.0324e-01 -6.7413e-02  4.1459e-02 -5.1002e-02 -3.2402e-02\n",
       "                8.1106e-03  7.0382e-02  9.5402e-02  2.3782e-02 -1.5212e-02\n",
       "              \n",
       "              (2 ,15,.,.) = \n",
       "               -1.2574e-01 -1.4500e-01 -7.0599e-02 -8.3193e-02 -7.1622e-02\n",
       "               -5.7605e-02 -1.4146e-01 -6.1310e-02 -1.3957e-01 -6.8010e-02\n",
       "               -8.5058e-02 -4.2320e-02 -5.9808e-02 -6.8566e-02 -7.6725e-02\n",
       "                3.9036e-03  8.2355e-02  8.3387e-02 -1.6945e-02  5.3289e-03\n",
       "               -1.7206e-02  5.7096e-03 -1.8564e-02 -1.1100e-02  7.4429e-02\n",
       "              ...   \n",
       "                   ⋮ \n",
       "              \n",
       "              (29,0 ,.,.) = \n",
       "                6.0901e-02 -6.4301e-02 -6.9337e-02 -9.8978e-02 -2.6025e-02\n",
       "                6.7106e-02 -5.5551e-02 -1.0268e-01  1.5028e-02  6.2728e-03\n",
       "                8.8403e-02 -1.1080e-01  2.0166e-02  2.5689e-02 -3.3334e-02\n",
       "               -2.6456e-02 -1.2377e-01  1.8085e-02 -7.0357e-02 -6.1899e-02\n",
       "                4.1195e-02 -6.7770e-02 -7.5135e-02 -2.2971e-02 -7.3067e-02\n",
       "              \n",
       "              (29,1 ,.,.) = \n",
       "               -7.1337e-02 -4.7931e-02 -9.9432e-02 -1.0468e-01 -2.8417e-02\n",
       "               -7.1082e-02 -6.1444e-02 -7.5938e-02 -2.0909e-02 -3.0094e-02\n",
       "               -2.9203e-02 -5.2353e-03 -9.6652e-02 -8.7704e-02 -3.0546e-02\n",
       "               -3.7951e-03 -5.8306e-04 -5.5138e-02 -6.6440e-02 -5.9279e-02\n",
       "               -1.2684e-01 -4.1777e-02  2.8601e-02  2.2467e-02 -1.1188e-02\n",
       "              \n",
       "              (29,2 ,.,.) = \n",
       "               -2.2357e-02 -5.8476e-02 -8.4185e-03 -3.9577e-02 -2.9327e-02\n",
       "               -6.2752e-02  2.0475e-02 -1.3107e-02  1.5855e-02 -7.7945e-02\n",
       "               -2.7349e-02 -2.3674e-03  1.9348e-02 -4.3907e-02 -1.0603e-01\n",
       "               -3.2955e-02  6.4542e-03  1.2195e-02 -6.1734e-02 -8.9763e-02\n",
       "                2.3197e-02  8.6210e-04  1.7253e-02  9.2642e-03 -5.1117e-02\n",
       "                 ...\n",
       "              \n",
       "              (29,13,.,.) = \n",
       "                7.9067e-02 -6.4349e-02 -1.9999e-03 -9.7547e-02 -2.9785e-02\n",
       "                1.2943e-01 -3.3893e-02 -1.5320e-02  1.6587e-02  8.2676e-03\n",
       "                1.2642e-01 -6.8053e-02  2.6489e-03 -6.2378e-02 -5.5702e-02\n",
       "                4.9004e-02 -1.6958e-01 -4.3497e-02 -8.0875e-02 -7.3633e-02\n",
       "                6.6270e-02 -5.4608e-02 -9.5083e-02 -5.7909e-02 -9.9488e-03\n",
       "              \n",
       "              (29,14,.,.) = \n",
       "                4.8884e-02 -4.3030e-02 -5.3367e-02  5.2600e-03 -3.1938e-02\n",
       "                4.0785e-02 -2.0455e-02 -9.7558e-02 -8.8141e-03 -5.8958e-02\n",
       "                5.6653e-02 -7.3683e-02 -1.0752e-01 -1.7589e-02 -2.2210e-02\n",
       "                5.2663e-02 -1.4491e-03 -8.8274e-02 -1.5525e-03 -2.8551e-03\n",
       "                1.4519e-02  8.2414e-03 -4.8404e-02 -5.4879e-02 -6.9262e-02\n",
       "              \n",
       "              (29,15,.,.) = \n",
       "               -4.8116e-02 -5.7270e-02 -5.0517e-02  7.6671e-04  4.6410e-03\n",
       "               -4.9353e-02 -4.4997e-02 -3.8685e-02 -6.1858e-02 -4.9383e-02\n",
       "                1.5974e-02 -4.9261e-02 -4.2484e-02 -7.2034e-02  1.8318e-02\n",
       "               -3.9082e-02 -2.1990e-02  1.2851e-02  1.8837e-02 -4.2611e-02\n",
       "               -3.2274e-02 -5.4827e-02 -4.1418e-02 -1.8741e-03  2.1840e-02\n",
       "                   ⋮ \n",
       "              \n",
       "              (30,0 ,.,.) = \n",
       "               -1.0211e-01 -8.8703e-02  7.8520e-03 -5.1303e-03 -2.6033e-03\n",
       "               -8.3468e-02 -6.8941e-02  3.2455e-02  5.2291e-02 -3.9572e-02\n",
       "               -6.4914e-02 -9.2829e-02  6.0954e-02 -8.2277e-02 -6.2691e-02\n",
       "               -5.4945e-02 -9.9336e-02 -4.4558e-02 -7.6155e-02 -2.3858e-02\n",
       "               -8.8575e-02 -6.0502e-02 -8.7127e-02 -8.0735e-02 -1.1767e-02\n",
       "              \n",
       "              (30,1 ,.,.) = \n",
       "               -1.4591e-01  8.8461e-03 -7.3774e-03 -2.5879e-02 -1.0927e-01\n",
       "                7.6149e-03  2.6535e-02 -1.0104e-02  5.0464e-03 -7.5843e-02\n",
       "               -5.6696e-02 -6.6190e-02 -8.1930e-02 -8.2412e-02 -2.8843e-02\n",
       "               -1.6359e-02 -4.2648e-02 -3.4161e-02 -8.3740e-02 -9.9332e-02\n",
       "               -6.7157e-02 -5.4128e-02 -6.6116e-02 -1.7789e-02 -4.4628e-02\n",
       "              \n",
       "              (30,2 ,.,.) = \n",
       "               -6.6008e-02 -1.0761e-01  2.5235e-03 -2.6083e-02 -6.8545e-02\n",
       "               -5.6184e-02 -3.3918e-02 -7.8457e-02 -1.0916e-01 -5.7524e-02\n",
       "               -4.5886e-02 -5.1061e-02 -8.2464e-02 -1.0185e-01 -4.7152e-03\n",
       "               -1.6011e-02 -1.0902e-01 -1.0400e-01 -5.7114e-02 -4.1276e-02\n",
       "                9.7981e-02 -3.0743e-02 -9.0309e-02 -6.7160e-02  8.5321e-02\n",
       "                 ...\n",
       "              \n",
       "              (30,13,.,.) = \n",
       "               -7.6582e-02 -6.4833e-02  1.5539e-02  5.0730e-02  8.3293e-02\n",
       "               -1.0838e-01 -7.5161e-02 -8.6559e-02  3.9880e-03  7.0980e-02\n",
       "               -4.8993e-02 -8.0421e-02 -9.8649e-02 -3.5420e-02 -2.6406e-02\n",
       "               -1.4214e-02 -6.9901e-02 -4.5363e-02 -6.3166e-02 -3.9975e-02\n",
       "               -5.3465e-02  6.6866e-02 -6.3146e-02 -2.7059e-02 -4.9703e-02\n",
       "              \n",
       "              (30,14,.,.) = \n",
       "               -1.2553e-01 -9.9866e-02 -1.1026e-01  4.1117e-03 -6.6961e-02\n",
       "               -1.2162e-01 -9.5686e-02 -4.8733e-02 -6.3136e-03 -1.2172e-01\n",
       "               -3.6620e-02 -1.0863e-01 -3.7789e-02 -7.5119e-02 -7.8890e-02\n",
       "               -5.2370e-02 -4.5904e-02 -6.6873e-02 -5.6867e-02 -3.8013e-02\n",
       "               -3.2650e-02 -9.1491e-02 -5.5652e-02 -7.9618e-02 -4.4247e-02\n",
       "              \n",
       "              (30,15,.,.) = \n",
       "               -8.0175e-02 -9.2674e-02 -7.1732e-02 -9.2237e-02 -5.2804e-02\n",
       "               -7.5922e-02 -4.3627e-02 -1.1271e-01 -4.3728e-02 -3.9311e-02\n",
       "               -4.8787e-02 -5.6322e-02 -3.5144e-02 -6.4626e-02 -5.5584e-02\n",
       "               -9.6438e-02 -7.3798e-02 -3.7121e-02 -9.1735e-02 -8.7069e-02\n",
       "               -6.6440e-02 -9.7890e-02 -3.3112e-02 -6.7769e-02 -5.2350e-02\n",
       "                   ⋮ \n",
       "              \n",
       "              (31,0 ,.,.) = \n",
       "               -3.0560e-02 -3.8738e-02 -4.2324e-02 -5.6224e-02 -8.4367e-02\n",
       "               -8.7302e-02 -9.5888e-02 -1.0160e-01 -7.2894e-02 -7.2527e-02\n",
       "               -1.1733e-01 -6.1069e-02 -6.7958e-02  2.7546e-02  1.6364e-02\n",
       "               -6.3665e-02 -5.3079e-02 -3.4844e-02 -3.6191e-02 -7.2763e-02\n",
       "               -1.6673e-02 -7.7136e-02 -7.5039e-02  2.7867e-02  2.6749e-03\n",
       "              \n",
       "              (31,1 ,.,.) = \n",
       "               -2.4444e-02 -6.4977e-02 -1.4141e-02 -3.0105e-02 -5.6035e-02\n",
       "               -9.5135e-03 -4.3560e-03 -4.9522e-02 -3.3112e-02 -7.1540e-03\n",
       "               -1.0836e-01 -5.3946e-02  9.9915e-03 -6.6047e-02 -2.9746e-02\n",
       "               -1.9892e-02 -4.4496e-02 -6.1274e-03 -3.4178e-03 -7.2220e-02\n",
       "                2.1503e-02 -4.0241e-02 -2.4179e-02  6.4435e-03 -5.0227e-02\n",
       "              \n",
       "              (31,2 ,.,.) = \n",
       "               -5.8015e-02 -1.6397e-02 -3.8089e-02 -6.8598e-02 -8.2683e-02\n",
       "               -1.0353e-01 -7.1253e-03 -5.9326e-03 -7.3107e-02 -1.9645e-02\n",
       "                5.1485e-03 -1.7031e-02 -2.8050e-02 -3.9879e-02 -8.2169e-02\n",
       "               -7.3953e-02 -8.5151e-02 -1.0323e-01 -4.7388e-02 -2.7616e-02\n",
       "               -2.8574e-02 -6.8918e-02 -2.1481e-02 -1.0281e-01 -1.4506e-02\n",
       "                 ...\n",
       "              \n",
       "              (31,13,.,.) = \n",
       "               -3.2890e-02 -1.1214e-01 -2.7912e-02 -3.5908e-02 -2.7484e-02\n",
       "               -7.7701e-02 -9.1977e-02 -9.1027e-02 -5.7949e-02  3.5310e-03\n",
       "               -4.4352e-02 -7.5306e-02 -2.8211e-02 -2.2258e-02  3.5997e-02\n",
       "               -5.2186e-02 -4.3398e-02 -7.9027e-02  8.0417e-03  4.7072e-03\n",
       "               -3.3408e-02  8.8951e-04 -1.0374e-02 -6.1669e-02  1.0165e-02\n",
       "              \n",
       "              (31,14,.,.) = \n",
       "               -4.1053e-02 -2.8057e-02 -9.4436e-02 -1.0007e-01 -9.8145e-02\n",
       "               -1.0645e-01 -4.2393e-02 -2.5066e-02 -7.7970e-02  1.1785e-02\n",
       "               -4.7206e-02 -1.0394e-01 -2.5304e-02 -6.0956e-02  8.7469e-03\n",
       "               -3.9373e-02 -4.9228e-02 -6.5902e-02  1.4669e-02 -3.5412e-03\n",
       "               -1.5572e-03 -7.0329e-02 -1.5230e-02  1.8186e-02 -4.7995e-02\n",
       "              \n",
       "              (31,15,.,.) = \n",
       "               -5.3997e-02 -6.3378e-02 -9.1927e-02 -6.1999e-02 -6.3294e-02\n",
       "               -1.2411e-02 -6.2654e-02 -8.2303e-03 -8.1931e-03 -7.3050e-02\n",
       "               -1.9246e-02 -7.6593e-02 -4.3140e-02 -1.0743e-01 -6.4381e-02\n",
       "               -3.5813e-02 -1.0125e-01 -7.4129e-02 -4.5661e-02 -2.2686e-02\n",
       "               -6.0099e-02 -9.2910e-03 -3.1402e-02 -8.4326e-02 -7.7406e-02\n",
       "              [torch.FloatTensor of size 32x16x5x5]), ('conv2.0.bias', \n",
       "              -0.0869\n",
       "              -0.0923\n",
       "              -0.1134\n",
       "              -0.0521\n",
       "              -0.0658\n",
       "              -0.0848\n",
       "              -0.0442\n",
       "              -0.1322\n",
       "               0.0181\n",
       "              -0.0077\n",
       "              -0.1089\n",
       "              -0.0452\n",
       "               0.0044\n",
       "              -0.0193\n",
       "              -0.0417\n",
       "              -0.0740\n",
       "               0.0293\n",
       "              -0.0601\n",
       "              -0.0444\n",
       "              -0.0682\n",
       "              -0.1153\n",
       "              -0.0660\n",
       "              -0.0579\n",
       "              -0.0047\n",
       "              -0.0573\n",
       "              -0.0348\n",
       "              -0.1109\n",
       "              -0.0381\n",
       "              -0.0025\n",
       "              -0.0371\n",
       "              -0.1237\n",
       "              -0.0412\n",
       "              [torch.FloatTensor of size 32]), ('out.weight', \n",
       "               7.9327e-02 -6.8459e-02 -5.9372e-02  ...  -7.2195e-02 -9.9787e-02 -9.5248e-02\n",
       "               5.8931e-02  4.0393e-02  5.8201e-02  ...  -7.5212e-02 -9.6722e-02  3.7431e-02\n",
       "              -3.6629e-02  5.5972e-02  9.8426e-02  ...   2.9732e-02  1.0237e-01  6.8106e-02\n",
       "                              ...                   ⋱                   ...                \n",
       "              -3.7192e-02 -8.3110e-02 -8.7846e-02  ...   1.3459e-02 -4.1460e-02 -5.3048e-02\n",
       "              -5.3871e-02 -3.8229e-02 -5.5514e-02  ...   6.9489e-02  3.0258e-02 -6.2133e-02\n",
       "              -7.0680e-02 -3.9400e-02 -3.4522e-02  ...   7.7063e-02  3.6777e-02 -4.0324e-02\n",
       "              [torch.FloatTensor of size 10x1568]), ('out.bias', \n",
       "              1.00000e-02 *\n",
       "                0.2831\n",
       "                7.6482\n",
       "               -6.9806\n",
       "               -0.5367\n",
       "               -2.3759\n",
       "               -2.9381\n",
       "               -2.7276\n",
       "               -2.2536\n",
       "                2.9768\n",
       "               -1.2600\n",
       "              [torch.FloatTensor of size 10])])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(cnn.state_dict(), './output/batch_1000_weight')\n",
    "cnn.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,0 ,.,.) = \n",
       "  8.6799e+09  2.2180e+10  2.9011e+10  4.5454e+10  8.0286e+10\n",
       " -9.7291e+10  1.3480e+11  5.9660e+10  4.8984e+10 -8.9234e+10\n",
       "  8.8004e+10  1.4051e+11  9.7416e+10 -1.3469e+11 -1.7229e+11\n",
       "  6.7406e+10  1.4972e+11 -8.2969e+10 -1.0029e+11  3.5183e+10\n",
       " -7.5476e+10  9.5747e+10  3.1972e+10 -2.0985e+10  8.4943e+09\n",
       "\n",
       "(1 ,0 ,.,.) = \n",
       "  6.9443e+10  9.5080e+10 -1.0977e+10  1.1815e+11  9.5633e+10\n",
       "  1.4220e+11  1.6201e+11  1.7087e+11 -3.0935e+10  1.4960e+11\n",
       " -7.6920e+10  6.3879e+10 -1.2752e+10  7.1446e+10 -3.5882e+10\n",
       " -1.5071e+11 -1.2168e+11 -3.6053e+10  3.4932e+10  5.9019e+09\n",
       " -1.6850e+11 -1.1059e+11 -1.8740e+11 -1.0286e+11 -3.3305e+10\n",
       "\n",
       "(2 ,0 ,.,.) = \n",
       " -1.3855e+11 -3.6419e+10  1.0760e+11 -5.3486e+10 -1.1007e+10\n",
       " -1.1282e+11  6.5434e+10  3.0046e+10  1.2042e+11 -1.4206e+11\n",
       " -2.5181e+11  7.8391e+10  1.4231e+11 -3.1319e+10 -5.6314e+10\n",
       " -1.3167e+11 -5.6847e+10  1.4253e+11  6.7386e+10 -3.7441e+10\n",
       " -1.6455e+11 -4.6314e+10  1.2896e+11  1.2616e+11 -6.6670e+10\n",
       "\n",
       "(3 ,0 ,.,.) = \n",
       "  7.2853e+09  8.6910e+10  6.3383e+10  1.1018e+11 -1.1527e+11\n",
       " -8.6835e+10 -9.6997e+10  2.3572e+10  1.1491e+11  1.1583e+11\n",
       " -1.9378e+11  5.3608e+10 -6.5268e+10  1.1866e+10  9.6524e+10\n",
       " -3.2927e+10 -1.1702e+11 -6.5442e+10 -3.4250e+10  8.9935e+10\n",
       "  4.8565e+10 -1.1007e+11 -5.5486e+10 -1.0509e+11 -1.5489e+11\n",
       "\n",
       "(4 ,0 ,.,.) = \n",
       "  4.4693e+10 -1.1233e+11 -1.2940e+11 -1.4181e+11 -7.8796e+10\n",
       " -2.1871e+11 -2.0780e+11 -1.6425e+11 -4.0408e+10  7.9850e+10\n",
       " -1.5825e+11 -8.5285e+10 -6.7446e+10 -1.4891e+09  1.0477e+10\n",
       "  1.2953e+11 -2.8801e+10  1.4361e+11  1.6630e+11  4.2783e+09\n",
       "  7.0535e+10  1.4799e+11 -1.9154e+10  1.3610e+11  2.6115e+10\n",
       "\n",
       "(5 ,0 ,.,.) = \n",
       " -4.5778e+10  1.2492e+11  2.4899e+10 -1.4197e+11  8.9768e+09\n",
       "  9.0576e+10  5.6180e+10  1.2081e+11  9.5276e+10  6.2409e+10\n",
       " -3.2981e+10 -6.5563e+10  4.4175e+10 -9.5283e+10  4.7148e+10\n",
       " -6.6024e+10  7.4681e+10  1.1396e+11 -5.9893e+10 -6.0171e+10\n",
       "  3.3606e+10  6.0699e+10  6.0030e+10 -1.2538e+11  1.8537e+10\n",
       "\n",
       "(6 ,0 ,.,.) = \n",
       "  1.3834e+11 -1.8207e+11 -3.0010e+11 -1.4091e+11  2.0403e+10\n",
       "  6.0158e+10 -1.3599e+11 -1.2587e+11  3.4340e+10  8.7283e+10\n",
       " -1.7527e+09 -9.1356e+10 -1.0601e+11  1.1582e+11  8.2664e+10\n",
       " -1.0239e+11 -3.2819e+10 -3.9097e+10  1.8171e+11 -9.0716e+10\n",
       " -8.5786e+10 -2.1329e+10  1.4312e+11  5.6842e+10 -1.9585e+11\n",
       "\n",
       "(7 ,0 ,.,.) = \n",
       " -4.0976e+10  1.2463e+11  4.8671e+10 -3.6198e+10 -2.5359e+10\n",
       "  3.5904e+10  4.0397e+10  1.0923e+11  8.1594e+10 -1.1901e+11\n",
       " -6.1412e+10  1.2260e+11  2.5011e+10  6.3754e+10 -4.3127e+10\n",
       "  5.0094e+10  1.4327e+11 -4.3861e+10  1.9843e+10 -8.8966e+10\n",
       "  3.9334e+10  3.5166e+10  1.2493e+11  1.3051e+11 -8.6549e+10\n",
       "\n",
       "(8 ,0 ,.,.) = \n",
       " -1.2482e+11 -2.5497e+11 -1.7961e+11 -1.4617e+11 -3.4909e+10\n",
       "  1.3594e+11 -4.2931e+08 -9.2169e+10 -9.0585e+10 -1.4335e+11\n",
       "  1.6434e+11  2.8599e+09  9.8934e+10 -7.7849e+10  9.9049e+10\n",
       " -5.4704e+10  8.0003e+10  3.9249e+10  1.6460e+11  3.5336e+09\n",
       "  1.0111e+11  9.8706e+10 -4.7053e+10  7.9718e+10  7.5945e+10\n",
       "\n",
       "(9 ,0 ,.,.) = \n",
       " -1.5747e+10  6.8563e+10  1.4586e+11  4.2400e+10  1.2757e+11\n",
       " -1.6631e+10  5.5405e+10  9.8703e+10  1.8658e+11  4.0543e+10\n",
       "  1.7362e+11  1.5353e+10  5.0390e+09 -5.9680e+10  1.3101e+11\n",
       " -9.7970e+09 -8.1936e+10 -9.8817e+10 -1.2728e+11 -5.6905e+10\n",
       " -1.7213e+11 -1.9161e+11 -1.5127e+11 -3.7717e+10 -3.6839e+10\n",
       "\n",
       "(10,0 ,.,.) = \n",
       "  1.1749e+11 -7.0183e+10  2.6620e+09 -5.6404e+10 -1.0102e+11\n",
       "  1.3694e+11  3.0830e+10  5.9172e+10  5.6288e+10  1.0123e+11\n",
       " -6.0962e+09  1.4076e+11  1.0793e+11  3.8154e+10  1.3756e+11\n",
       " -1.6138e+11  7.6931e+10  1.4193e+10 -3.0936e+10  7.9717e+10\n",
       " -4.0153e+10 -1.9051e+11 -1.2853e+11 -7.6004e+10 -8.7214e+10\n",
       "\n",
       "(11,0 ,.,.) = \n",
       " -1.3003e+11  4.5146e+10  1.6714e+10 -8.2299e+10 -1.3273e+11\n",
       " -1.1689e+11 -6.6850e+10 -6.3182e+10 -1.5421e+11 -1.7716e+10\n",
       "  8.0722e+10  4.5915e+10 -4.6999e+10 -1.5720e+11 -3.3935e+10\n",
       "  1.3321e+11 -8.1735e+10  4.4512e+10  2.8822e+10 -4.6625e+10\n",
       "  6.8056e+10  1.2685e+11  3.1663e+10 -1.2215e+11 -1.1423e+11\n",
       "\n",
       "(12,0 ,.,.) = \n",
       "  1.4523e+11  1.5328e+11  1.4196e+11  5.2837e+10 -1.3529e+11\n",
       "  9.7356e+10  1.1450e+10 -1.2146e+10 -4.9338e+09 -2.0401e+11\n",
       "  1.2910e+11 -5.1874e+10 -1.4105e+10 -1.7711e+11 -1.3850e+11\n",
       " -1.1998e+10  2.3569e+10 -8.8824e+09 -2.0235e+11 -1.4187e+11\n",
       "  1.0115e+11 -3.0196e+10 -2.4017e+11 -9.2771e+10 -8.6933e+10\n",
       "\n",
       "(13,0 ,.,.) = \n",
       "  7.0947e+10  1.0434e+11  6.5574e+09 -2.7523e+10  2.3430e+10\n",
       "  7.0989e+10  1.5799e+10  2.0757e+10  4.2032e+10 -1.3318e+11\n",
       "  1.4154e+11  1.3584e+11  8.1580e+10 -1.1561e+11 -8.6467e+10\n",
       "  1.3398e+11  6.9281e+10 -9.8535e+10 -9.6742e+10 -3.6073e+10\n",
       "  8.1035e+10  1.1879e+11 -4.7717e+10 -7.3493e+10  3.0009e+10\n",
       "\n",
       "(14,0 ,.,.) = \n",
       "  1.3264e+11  7.4915e+10  9.1262e+10  9.2867e+10  1.5265e+11\n",
       "  1.1315e+11 -2.7601e+10 -6.1151e+10  6.5774e+10  4.7372e+08\n",
       " -1.6591e+10  5.6776e+10 -8.2345e+10 -1.3823e+11 -6.3894e+10\n",
       "  5.0435e+10 -1.3362e+11 -1.4405e+11 -1.0545e+11 -4.9386e+10\n",
       "  6.1064e+10  3.5527e+10 -1.9505e+08  5.0097e+10 -3.0090e+10\n",
       "\n",
       "(15,0 ,.,.) = \n",
       " -2.2461e+10  8.3015e+10 -9.2708e+10  3.9439e+10 -6.5606e+10\n",
       "  5.8235e+10 -7.9328e+10 -1.5647e+10  5.3965e+10  1.5381e+11\n",
       " -9.9433e+10  4.0886e+10  4.9477e+10  1.0376e+11  7.2089e+10\n",
       " -1.7461e+11 -1.1617e+11 -4.3423e+10 -7.0550e+10 -6.5620e+10\n",
       " -5.9761e+10 -1.3902e+10 -1.9014e+11 -1.1594e+11 -1.4873e+11\n",
       "[torch.FloatTensor of size 16x1x5x5]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.state_dict()['conv1.0.weight'] *=2\n",
    "cnn.state_dict()['conv1.0.weight']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
